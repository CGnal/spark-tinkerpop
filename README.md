Spark-Tinkerpop
=======

Spark-Tinkerpop is an enrichment library supplementing the [Spark GraphX](http://spark.apache.org/graphx/) framework. While serving as an integration layer with [Apache Tinkerpop](https://tinkerpop.apache.org), it also adds saving and loading functionality against various serialization methods, including Java, Kryo and JSON, besides also providing APIs for implementing support for other graph processing engines that could benefit from Spark's powerful processing paradigms. 

Quickstart
-------

The main functionality that Spark-Tinkerpop brings to the table revolves around persisting the data generated by GraphX, naturally allowing for incremental graph processing, but also to integrate with other graph processing engines such as (Titan)[http://titan.thinkaurelius.com], (Neo4J)[https://neo4j.com] and (OrientDB)[http://orientdb.com/orientdb/].

Graph persistence comes in a few different forms in Spark-Tinkerpop:

#### Java

The simplest way to read and write graph data is of course to use plain Java serialization; this calls for a few considerations on speed and efficiency, as well as backward-compatibility in case the code changes between graph-processing increments.

```scala
...

import org.cgnal.graphe._

...
// save graph at [location]
graph.save(location).asJava
...
// load graph from [location]: Vertices are of type V, Edges are of type E
sparkContext.load[V, E](location).asJava

```

#### Kryo

Given the limitations of plain Java serialization, using [Kryo](https://github.com/EsotericSoftware/kryo) as an alternative binary serialization format can often prove beneficial in both performance and flexibility when it comes to adapting an existing application and its data to future changes. Kryo serialization entails a little more work than Java in that one must provide custom serializers, which is what provides the flexibility in the end.

Spark-Tinkerpop provides [a single point of entry](core/src/main/scala/org/apache/spark/graphx/serialization/kryo/KryoRegistry.scala) for registering custom serializers easily along with all the required serializers for the more ubiquitous Scala classes such as `Tuple` classes. The following snippets are inspired from actual [test code](core/src/test/scala/org/apache/spark/graphx/serialization/kryo).

```scala

sealed trait Gender

case object Male extends Gender

case object Female extends Gender

case class User(id: String, name: String, gender: Gender)

...

object GenderSerializer extends Serializer[Gender] {
	
  def write(kryo: Kryo, output: Output, gender: Gender) = gender match {
    case Male   => output.writeString { "male"   }
    case Female => output.writeString { "female" }
  } 

  def read(kryo: Kryo, input: Input, genderClass: Class[Gender]) = input.readString() match {
    case "male"   => Male
    case "female" => Female
    case other    => throw new IllegalArgumentException(s"Invalid string for gender [$other]")
  }

}

object UserSerializer extends Serializer[User] {

  def write(kryo: Kryo, output: Output, user: User) = {
    output.writeLong(user.id)
    output.writeString(user.name)
    kryo.writeObject(output, user.gender, GenderSerializer)
  }

  def read(kryo: Kryo, input: Input, userClass: Class[User]) = User(
    id     = input.readLong(),
    name   = input.readString(),
    gender = kryo.readObject(input, classOf[Gender], GenderSerializer)
  )

}
```

Assuming we have the above serializers, then the registry would look something similar to:

```scala
...

import org.apache.spark.graphx.serialization.kryo.ClassRegistrable

...
object CustomKryoRegistry extends KryoRegistry with Serializable {

  protected def registry =
    { classOf[User]   serializeWith UserSerializer   } :+
    { classOf[Gender] serializeWith GenderSerializer }

}
```

What's left to do is then to provide this registry in the implicit scope before we can call the functions to save or load in exactly the same way as we did for Java above:

```scala
...

import org.cgnal.graphe._

...
// save graph at [location]
graph.save(location).asKryo
...
// load graph from [location]: Vertices are of type V, Edges are of type E
sparkContext.load[V, E](location).asKryo
```

#### GraphSON


